{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ETL.Functions.api_yelp import *\n",
    "import pandas as pd\n",
    "\n",
    "ruta_bucket = \"gs://bucket-steakhouses2\"\n",
    "nombre_archivo = \"review.parquet\"\n",
    "\n",
    "business_ids = ['4s4KAGJgtpZn9cgtdWcOmw', 'FN1sLS5hfMydZX94SYD1QA', 'uamAUnL4ox5uXvjquY9wLg']\n",
    "\n",
    "reviews = []\n",
    "for business_id in business_ids:\n",
    "    reviews += API_reviews(business_id)\n",
    "\n",
    "    df_reviews_api = pd.DataFrame(reviews)\n",
    "    df_reviews_existentes = reviews_existentes()\n",
    "\n",
    "\n",
    "    df_reviews_nuevas  = df_reviews_api.loc[~df_reviews_api['review_id'].isin(df_reviews_existentes['review_id'])]\n",
    "\n",
    "can_reviews = df_reviews_nuevas.shape[0]\n",
    "print (df_reviews_nuevas.shape[0], 'nuevas reviews')\n",
    "if can_reviews>0:\n",
    "    archivo = f'{ruta_bucket}/filtrado/{nombre_archivo}'\n",
    "    df_reviews_nuevas.to_parquet(archivo, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_nuevas['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_nuevas.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Create a credentials object\n",
    "credentials, _ = google.auth.default()\n",
    "\n",
    "# Create a BigQuery client\n",
    "client = bigquery.Client(credentials=credentials)\n",
    "\n",
    "\n",
    "\n",
    "# Crear tablas en BigQuery a partir de archivos\n",
    "tabla = 'review'\n",
    "job_config = bigquery.LoadJobConfig(source_format=bigquery.SourceFormat.PARQUET,)\n",
    "\n",
    "table_id = f'steakhouses2.yelp.{tabla}'\n",
    "uri = f\"{ruta_bucket}/filtrado/{tabla}.parquet\"\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  # API request.\n",
    "\n",
    "load_job.result()  # Espera que se complete el trabajo.\n",
    "destination_table = client.get_table(table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Create a credentials object\n",
    "credentials, _ = google.auth.default()\n",
    "\n",
    "# Create a BigQuery client\n",
    "client = bigquery.Client(credentials=credentials)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Insert data from DataFrame into BigQuery\n",
    "table_ref = client.get_table('steakhouses2.yelp.review')\n",
    "job = client.load_table_from_dataframe(df_reviews_nuevas, table_ref)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def check_file_exists(bucket_name, file_name):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "\n",
    "    exists = blob.exists()\n",
    "    return exists\n",
    "\n",
    "ruta_bucket = \"gs://bucket-steakhouses2\"\n",
    "\n",
    "print (check_file_exists(ruta_bucket, '/crudo/review.parquet'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def existe_archivo(bucket_name, archivo):\n",
    "    # Inicializa el cliente de GCS\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Obtiene una referencia al bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "\n",
    "    # Verifica si el archivo existe en la carpeta del bucket\n",
    "    blob = bucket.blob(archivo)\n",
    "    return blob.exists()\n",
    "\n",
    "bucket_name = \"bucket-steakhouses2\"\n",
    "archivo = 'crudo/yelp/review.parquet'\n",
    "\n",
    "print (existe_archivo(bucket_name, archivo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ETL.Functions.api_yelp import *\n",
    "business_a_consultar(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultimo = '-H8ryT3wGAoHMa9NeZ4tNQ'\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Create a credentials object\n",
    "credentials, _ = google.auth.default()\n",
    "\n",
    "# Create a BigQuery client\n",
    "client = bigquery.Client(credentials=credentials)\n",
    "\n",
    "\n",
    "table_ref = client.get_table('steakhouses2.yelp.review')\n",
    "\n",
    "\n",
    "\n",
    "# Crear registro tabla auditoria\n",
    "#ultimo = df_reviews_nuevas.iloc[-1]['business_id']\n",
    "audit = client.get_table('steakhouses2.yelp.audit')\n",
    "row = bigquery.Row([datetime.now(), 'review', \"Carga incremental API\", 0, table_ref.num_rows, None, 'business_id', ultimo], [])\n",
    "client.insert_rows(audit, [row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ozzy/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ozzy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerías a usar\n",
    "import pandas as pd\n",
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "nltk.download('vader_lexicon') # Descarga vader_lexicon para el análisis de sentimientos.\n",
    "nltk.download('punkt') # Descarga punkt, un modelo de toquenización que divide el texto en palabras individuales\n",
    "\n",
    "def sentimientos_Yelp (yelp_as):\n",
    "    # Convertir las estrellas en score\n",
    "    def to_sentiment(rating):\n",
    "        \n",
    "        rating = int(rating)\n",
    "        \n",
    "        # Convert to class\n",
    "        if rating <= 2:\n",
    "            return 0\n",
    "        elif rating == 3:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "    # Apply to the dataset \n",
    "    yelp_as['sentiment'] = yelp_as.stars.apply(to_sentiment)\n",
    "\n",
    "    # Función para clasificar los textos\n",
    "    def classify_text(text):\n",
    "        if 'food' in text.lower():\n",
    "            return 'comida'\n",
    "        elif 'service' in text.lower():\n",
    "            return 'servicio'\n",
    "        elif 'ambience' in text.lower():\n",
    "            return 'ambiente'\n",
    "        return 'otro'\n",
    "\n",
    "    # Aplica la función a la columna 'text' y crea una nueva columna 'temas'\n",
    "    yelp_as['temas'] = yelp_as['text'].apply(classify_text)\n",
    "\n",
    "\n",
    "\n",
    "    analizador = SentimentIntensityAnalyzer()\n",
    "    def Puntaje_Sentimiento(text):\n",
    "        tokens = word_tokenize(text)  # Tokenizar el texto\n",
    "        scores = analizador.polarity_scores(text)  # Obtener los puntajes de sentimiento\n",
    "        return scores['compound']  # Retornar el puntaje compuesto\n",
    "\n",
    "    yelp_as['score_sentimientos'] = yelp_as['text'].apply(Puntaje_Sentimiento)\n",
    "\n",
    "    yelp_as['categorizacion'] = 0  # Inicializamos con 0 por defecto\n",
    "\n",
    "    # Aplicamos las condiciones para actualizar los valores en la nueva columna\n",
    "    yelp_as.loc[(yelp_as['score_sentimientos'] > -1) & (yelp_as['score_sentimientos'] < 0), 'categorizacion'] = 0\n",
    "    yelp_as.loc[yelp_as['score_sentimientos'] == 0, 'categorizacion'] = 1\n",
    "    yelp_as.loc[(yelp_as['score_sentimientos'] >= 0.1) & (yelp_as['score_sentimientos'] <= 1), 'categorizacion'] = 2\n",
    "\n",
    "\n",
    "    # Supongamos que tienes un DataFrame llamado 'yelp_as'\n",
    "    yelp_as['nuevos_scores'] = yelp_as['score_sentimientos'] + 1\n",
    "\n",
    "    return yelp_as\n",
    "\n",
    "# Llamar a la función\n",
    "ruta_bucket = \"gs://bucket-steakhouses2\"\n",
    "df = pd.read_parquet(f'{ruta_bucket}/filtrado/yelp/review.parquet')\n",
    "df = sentimientos_Yelp(df)\n",
    "\n",
    "# Exporta el dataframe a un archivo Parquet\n",
    "df.to_parquet(f'{ruta_bucket}/filtrado/yelp/Yelp_Analisis_Sentimiento.parquet', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
